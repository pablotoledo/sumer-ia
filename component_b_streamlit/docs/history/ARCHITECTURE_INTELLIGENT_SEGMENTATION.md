# Arquitectura: SegmentaciÃ³n Inteligente con GPT-4.1

## Objetivo
Reemplazar la segmentaciÃ³n programÃ¡tica simple por un agente inteligente que aproveche GPT-4.1 (1M token context) para identificar puntos de corte semÃ¡nticos Ã³ptimos.

## Ventajas del Nuevo DiseÃ±o

### 1. Calidad Superior
- âœ… Cortes en transiciones naturales de tema
- âœ… Cada segmento es una unidad lÃ³gica coherente
- âœ… Mejor comprensiÃ³n del contenido por el modelo
- âœ… Metadata Ãºtil (tÃ­tulos, keywords, tipos)

### 2. Aprovecha GPT-4.1
- âœ… 24k palabras = ~31k tokens (3% del lÃ­mite de 1M)
- âœ… Una sola llamada para todo el anÃ¡lisis
- âœ… Costo mÃ­nimo adicional (~$0.03 por 24k palabras)

### 3. Contexto Limpio
- âœ… Cada segmento se procesa con contexto fresco
- âœ… No hay "arrastre" de conversaciÃ³n anterior
- âœ… Procesamiento paralelo sin interferencias

---

## Arquitectura Propuesta

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FASE 1: ANÃLISIS                         â”‚
â”‚                   (1 llamada, contexto Ãºnico)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input: Todo el contenido (24k palabras)
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Agente: intelligent_segmenter          â”‚
â”‚   Modelo: GPT-4.1                        â”‚
â”‚   Contexto: ~31k tokens (3% del lÃ­mite)  â”‚
â”‚                                          â”‚
â”‚   InstrucciÃ³n:                           â”‚
â”‚   "Analiza este contenido y determina   â”‚
â”‚    los mejores puntos de segmentaciÃ³n   â”‚
â”‚    para dividirlo en unidades            â”‚
â”‚    semÃ¡nticamente coherentes de          â”‚
â”‚    ~2500 palabras cada una"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Output: JSON estructurado
{
  "total_words": 24000,
  "recommended_segments": 10,
  "segments": [
    {
      "id": 1,
      "start_word": 0,
      "end_word": 2450,
      "word_count": 2450,
      "topic": "IntroducciÃ³n: Identificar empresas de calidad",
      "keywords": ["Warren Buffett", "calidad", "valoraciÃ³n", "BMW", "Renault"],
      "section_type": "introduction",
      "key_concepts": [
        "Diferencia entre precio y calidad",
        "Herramientas de anÃ¡lisis (Morningstar, TKR)"
      ],
      "transition_type": "natural_break"
    },
    {
      "id": 2,
      "start_word": 2450,
      "end_word": 4920,
      "word_count": 2470,
      "topic": "MÃ©tricas clave: ROE, mÃ¡rgenes y ratios",
      "keywords": ["ROE", "ROIC", "mÃ¡rgenes operativos", "free cash flow"],
      "section_type": "main_content",
      "key_concepts": [
        "Retorno sobre capital invertido",
        "ComparaciÃ³n entre BMW, Ford y Renault"
      ],
      "transition_type": "topic_change"
    },
    // ... 8 segmentos mÃ¡s
  ],
  "format_detected": "educational_linear",
  "recommended_agent": "simple_processor"
}


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASE 2: PROCESAMIENTO                        â”‚
â”‚              (10 llamadas, contextos independientes)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Para cada segmento del JSON:
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Agente: simple_processor               â”‚
â”‚   Modelo: GPT-4.1                        â”‚
â”‚   Contexto: LIMPIO (nueva sesiÃ³n)        â”‚
â”‚   Input: ~2500 palabras del segmento    â”‚
â”‚                                          â”‚
â”‚   Metadata adicional del segmento:      â”‚
â”‚   - Topic: "..."                         â”‚
â”‚   - Keywords: [...]                      â”‚
â”‚   - Key concepts: [...]                  â”‚
â”‚   - Position: 2/10                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Output: Contenido procesado con Q&A


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FASE 3: ENSAMBLADO                         â”‚
â”‚                     (sin llamadas LLM)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Combinar todos los segmentos procesados en documento final
```

---

## ImplementaciÃ³n Detallada

### 1. Nuevo Agente: `intelligent_segmenter`

**UbicaciÃ³n**: `src/enhanced_agents.py`

```python
@fast.agent(
    name="intelligent_segmenter",
    model=DEFAULT_MODEL,  # GPT-4.1
    instruction="""You are an expert content analyzer specializing in identifying optimal segmentation points.

TASK: Analyze the provided content and determine the best way to divide it into semantically coherent segments.

REQUIREMENTS:
1. Each segment should be ~2500 words (range: 2000-3000 words)
2. Break at natural topic transitions, not mid-concept
3. Identify the main topic/theme of each segment
4. Extract key concepts and keywords for each segment
5. Classify segment type: introduction, main_content, example, conclusion, transition

OUTPUT FORMAT (valid JSON):
{
  "total_words": <int>,
  "recommended_segments": <int>,
  "segments": [
    {
      "id": <int>,
      "start_word": <int>,
      "end_word": <int>,
      "word_count": <int>,
      "topic": "<concise topic description>",
      "keywords": ["<keyword1>", "<keyword2>", ...],
      "section_type": "<introduction|main_content|example|conclusion|transition>",
      "key_concepts": ["<concept1>", "<concept2>", ...],
      "transition_type": "<natural_break|topic_change|speaker_change|section_end>"
    }
  ],
  "format_detected": "<educational_linear|meeting_dialogue|technical_document>",
  "recommended_agent": "<simple_processor|meeting_processor>"
}

IMPORTANT:
- Ensure start_word and end_word cover ALL content with no gaps
- The last segment's end_word must equal total_words
- Avoid segments smaller than 1000 words unless necessary
- Prefer clean breaks at paragraph or sentence boundaries
- If content has clear section markers (headers, transitions), use them
"""
)
def intelligent_segmenter():
    pass
```

### 2. Nueva FunciÃ³n: `adaptive_segment_content_v2`

**UbicaciÃ³n**: `src/enhanced_agents.py`

```python
import json
from typing import Dict, Any

async def adaptive_segment_content_v2(content: str) -> Tuple[List[Dict[str, Any]], str]:
    """
    Use GPT-4.1 to intelligently segment content based on semantic analysis.
    Returns (segment_metadata_list, recommended_agent) tuple.

    This replaces the programmatic segmentation with AI-driven analysis.
    """
    words = content.split()
    total_words = len(words)

    print(f"\nğŸ§  INTELLIGENT SEGMENTATION (GPT-4.1)")
    print(f"   â€¢ Total words: {total_words:,}")
    print(f"   â€¢ Analyzing content for optimal segmentation...")

    # Call the intelligent_segmenter agent
    async with fast.run() as agent_instance:
        # Send the full content for analysis
        segmentation_prompt = f"""Analyze and segment the following content:

CONTENT ({total_words} words):
{content}

Provide a JSON segmentation plan following the specified format."""

        result_json = await agent_instance.intelligent_segmenter.send(segmentation_prompt)

    # Parse the JSON response
    try:
        segmentation_plan = json.loads(result_json)
    except json.JSONDecodeError as e:
        print(f"âš ï¸  JSON parsing failed: {e}")
        print(f"   Falling back to programmatic segmentation...")
        # Fallback to old method
        return adaptive_segment_content(content)

    # Validate the segmentation plan
    segments_metadata = segmentation_plan.get('segments', [])
    recommended_agent = segmentation_plan.get('recommended_agent', 'simple_processor')

    print(f"   â€¢ Segments identified: {len(segments_metadata)}")
    print(f"   â€¢ Format detected: {segmentation_plan.get('format_detected', 'unknown')}")
    print(f"   â€¢ Recommended agent: {recommended_agent}")

    # Extract actual text for each segment
    enriched_segments = []
    for seg_meta in segments_metadata:
        start = seg_meta['start_word']
        end = seg_meta['end_word']

        segment_words = words[start:end]
        segment_text = ' '.join(segment_words)

        # Enrich with metadata
        enriched_segments.append({
            'content': segment_text,
            'metadata': seg_meta
        })

        print(f"   â€¢ Segment {seg_meta['id']}: {seg_meta['word_count']} words - {seg_meta['topic'][:60]}...")

    return enriched_segments, recommended_agent
```

### 3. Modificar `AgentInterface.process_content`

**UbicaciÃ³n**: `streamlit_app/components/agent_interface.py`

**Cambios clave:**

```python
async def process_content(
    self,
    content: str,
    documents: Optional[List[str]] = None,
    progress_callback=None,
    agent_override: Optional[str] = None,
    use_intelligent_segmentation: bool = True  # NUEVO PARÃMETRO
) -> Dict[str, Any]:
    """
    Procesa contenido usando FastAgent.

    Args:
        use_intelligent_segmentation: Si True, usa GPT-4.1 para segmentar.
                                      Si False, usa mÃ©todo programÃ¡tico.
    """

    if not await self._initialize_agents():
        raise Exception("No se pudieron inicializar los agentes")

    try:
        # PASO 1: SegmentaciÃ³n (con o sin IA)
        if progress_callback:
            progress_callback("Analizando contenido para segmentaciÃ³n...", 0.1)

        if use_intelligent_segmentation and len(content.split()) > 3000:
            # Usar agente inteligente para contenido grande
            print("ğŸ§  Using AI-powered intelligent segmentation")
            enriched_segments, recommended_agent = await self._intelligent_segment_with_ai(content)
        else:
            # Usar mÃ©todo programÃ¡tico para contenido pequeÃ±o
            print("ğŸ“ Using programmatic segmentation")
            segments, recommended_agent = self._adaptive_segment(content)
            enriched_segments = [{'content': seg, 'metadata': {}} for seg in segments]

        # Usar agent_override si se especifica
        if agent_override:
            recommended_agent = agent_override

        if progress_callback:
            progress_callback(f"SegmentaciÃ³n completa: {len(enriched_segments)} segmentos", 0.2)

        # PASO 2: Configurar contexto multimodal
        multimodal_context = self._prepare_multimodal_context(documents)

        # PASO 3: Procesar cada segmento con CONTEXTO LIMPIO
        processed_segments = []
        total_segments = len(enriched_segments)

        # Seleccionar agente
        if recommended_agent == "meeting_processor":
            agent = self._meeting_agent
        else:
            agent = self._fast_agent

        for i, enriched_segment in enumerate(enriched_segments):
            segment_content = enriched_segment['content']
            segment_metadata = enriched_segment.get('metadata', {})

            if progress_callback:
                progress = 0.2 + (0.7 * (i + 1) / total_segments)
                topic = segment_metadata.get('topic', f'Segmento {i + 1}')
                progress_callback(f"Procesando: {topic[:50]}...", progress)

            # IMPORTANTE: Cada iteraciÃ³n crea una NUEVA sesiÃ³n con contexto limpio
            try:
                async with agent.run() as agent_instance:  # Nueva sesiÃ³n = contexto limpio
                    # Construir prompt con metadata enriquecida
                    segment_prompt = self._build_segment_prompt(
                        segment_content=segment_content,
                        segment_number=i + 1,
                        total_segments=total_segments,
                        metadata=segment_metadata,
                        multimodal_context=multimodal_context
                    )

                    # Procesar con retry
                    result = await self._rate_limit_handler.execute_with_retry(
                        agent_instance.simple_processor.send,
                        segment_prompt
                    )

                    processed_segments.append({
                        'segment_number': i + 1,
                        'original_content': segment_content,
                        'processed_content': result,
                        'agent_used': recommended_agent,
                        'metadata': segment_metadata
                    })

            except Exception as e:
                st.warning(f"Error procesando segmento {i + 1}: {e}")
                processed_segments.append({
                    'segment_number': i + 1,
                    'original_content': segment_content,
                    'processed_content': f"Error: {e}",
                    'agent_used': recommended_agent,
                    'metadata': segment_metadata,
                    'error': True
                })

        # PASO 4: Ensamblar documento final
        if progress_callback:
            progress_callback("Ensamblando documento final...", 0.95)

        final_document = self._assemble_final_document(
            processed_segments,
            content,
            documents
        )

        if progress_callback:
            progress_callback("Â¡Procesamiento completado!", 1.0)

        return {
            'success': True,
            'document': final_document,
            'segments': processed_segments,
            'agent_used': recommended_agent,
            'total_segments': total_segments,
            'retry_count': self._rate_limit_handler.retry_count,
            'original_content': content,
            'multimodal_files': documents or [],
            'segmentation_method': 'intelligent_ai' if use_intelligent_segmentation else 'programmatic'
        }

    except Exception as e:
        # ... error handling ...


async def _intelligent_segment_with_ai(self, content: str) -> Tuple[List[Dict[str, Any]], str]:
    """Wrapper para llamar a la funciÃ³n de segmentaciÃ³n inteligente."""
    from src.enhanced_agents import adaptive_segment_content_v2
    return await adaptive_segment_content_v2(content)


def _build_segment_prompt(
    self,
    segment_content: str,
    segment_number: int,
    total_segments: int,
    metadata: Dict[str, Any],
    multimodal_context: str
) -> str:
    """
    Construye el prompt para procesar un segmento, incluyendo metadata Ãºtil.
    """
    prompt_parts = []

    # Header con posiciÃ³n
    prompt_parts.append(f"SEGMENTO {segment_number} de {total_segments}")

    # Metadata si estÃ¡ disponible
    if metadata:
        prompt_parts.append("\nMETADATA DEL SEGMENTO:")
        if 'topic' in metadata:
            prompt_parts.append(f"â€¢ Tema principal: {metadata['topic']}")
        if 'keywords' in metadata:
            prompt_parts.append(f"â€¢ Palabras clave: {', '.join(metadata['keywords'])}")
        if 'key_concepts' in metadata:
            prompt_parts.append(f"â€¢ Conceptos clave: {', '.join(metadata['key_concepts'])}")
        if 'section_type' in metadata:
            prompt_parts.append(f"â€¢ Tipo de secciÃ³n: {metadata['section_type']}")

    # Contenido del segmento
    prompt_parts.append(f"\nCONTENIDO:\n{segment_content}")

    # Contexto multimodal
    if multimodal_context:
        prompt_parts.append(f"\n{multimodal_context}")

    return '\n'.join(prompt_parts)
```

---

## Contexto Limpio: CÃ³mo Funciona

### Mecanismo de Limpieza

En fast-agent, **cada vez que se usa `async with agent.run() as agent_instance:`**, se crea una **nueva sesiÃ³n** con:

1. âœ… **Contexto vacÃ­o** - Sin historial de conversaciÃ³n
2. âœ… **Estado fresco** - Sin memoria de llamadas anteriores
3. âœ… **Independencia total** - Cada segmento se procesa aislado

### CÃ³digo Actual vs Nuevo

**âŒ ANTES (contexto compartido):**
```python
# UNA SOLA sesiÃ³n para TODOS los segmentos
async with agent.run() as agent_instance:
    for segment in segments:
        # Todos comparten el mismo contexto
        result = await agent_instance.simple_processor.send(segment)
```

**âœ… AHORA (contexto limpio):**
```python
# NUEVA sesiÃ³n para CADA segmento
for segment in segments:
    async with agent.run() as agent_instance:  # Nueva sesiÃ³n
        # Contexto limpio, sin memoria del anterior
        result = await agent_instance.simple_processor.send(segment)
```

### VerificaciÃ³n

Puedes verificar que el contexto se limpia observando los logs de fast-agent:
- Cada `agent.run()` inicia una nueva sesiÃ³n
- El contador de `turns` se resetea a 1 en cada segmento
- No hay "arrastre" de tokens de contexto entre segmentos

---

## ConfiguraciÃ³n y Uso

### Streamlit UI: Toggle para elegir mÃ©todo

En la pÃ¡gina de procesamiento, agregar:

```python
# En show_input_tab()
st.subheader("ğŸ§  MÃ©todo de SegmentaciÃ³n")

segmentation_method = st.radio(
    "Â¿CÃ³mo dividir el contenido?",
    options=[
        "ğŸ§  Inteligente (GPT-4.1 analiza y segmenta)",
        "ğŸ“ ProgramÃ¡tico (divisiÃ³n fija cada 2500 palabras)"
    ],
    help="""
    Inteligente: GPT-4.1 analiza tu contenido y encuentra los mejores puntos
    de corte semÃ¡nticos. Recomendado para >5000 palabras.

    ProgramÃ¡tico: DivisiÃ³n simple cada 2500 palabras. MÃ¡s rÃ¡pido pero menos preciso.
    """
)

st.session_state.use_intelligent_segmentation = (
    "Inteligente" in segmentation_method
)
```

En `process_content()`:
```python
result = run_async_in_streamlit(
    agent_interface.process_content(
        content=content,
        documents=document_paths,
        progress_callback=None,
        agent_override=selected_agent,
        use_intelligent_segmentation=st.session_state.get('use_intelligent_segmentation', True)
    )
)
```

---

## EstimaciÃ³n de Costos

### GPT-4.1 Pricing (Azure/OpenAI)
- Input: ~$2.50 por 1M tokens
- Output: ~$10.00 por 1M tokens

### Para contenido de 24,000 palabras (~31k tokens):

**Fase 1 - SegmentaciÃ³n Inteligente:**
- Input: 31k tokens Ã— $2.50/1M = **$0.078**
- Output: ~2k tokens Ã— $10/1M = **$0.020**
- **Subtotal Fase 1: $0.10**

**Fase 2 - Procesamiento (10 segmentos):**
- Input: 10 Ã— 3.3k tokens = 33k Ã— $2.50/1M = **$0.083**
- Output: 10 Ã— 2k tokens = 20k Ã— $10/1M = **$0.200**
- **Subtotal Fase 2: $0.28**

**TOTAL: ~$0.38 por documento de 24k palabras**

**vs MÃ©todo Anterior (30 segmentos pequeÃ±os):**
- Input: 30 Ã— 1.1k = 33k tokens = $0.083
- Output: 30 Ã— 1k = 30k tokens = $0.300
- **Total anterior: $0.38**

âœ… **Costo similar, pero MUCHA mejor calidad**

---

## Rollout Plan

### Fase 1: ImplementaciÃ³n (Sin tocar cÃ³digo existente)
1. âœ… Crear `intelligent_segmenter` agent en `enhanced_agents.py`
2. âœ… Crear `adaptive_segment_content_v2()` function
3. âœ… Mantener `adaptive_segment_content()` como fallback

### Fase 2: IntegraciÃ³n
1. âœ… Modificar `AgentInterface.process_content()` con parÃ¡metro `use_intelligent_segmentation`
2. âœ… Implementar `_intelligent_segment_with_ai()` y `_build_segment_prompt()`
3. âœ… Agregar UI toggle en Streamlit

### Fase 3: Testing
1. âœ… Test con contenido pequeÃ±o (< 3k palabras) â†’ debe usar mÃ©todo programÃ¡tico
2. âœ… Test con contenido mediano (5-10k palabras) â†’ debe usar IA, pocos segmentos
3. âœ… Test con contenido grande (24k palabras) â†’ debe usar IA, ~10 segmentos
4. âœ… Verificar contexto limpio (logs muestran nueva sesiÃ³n cada vez)
5. âœ… Validar JSON parsing y fallback

### Fase 4: Production
1. âœ… Default: `use_intelligent_segmentation=True` para contenido > 5k palabras
2. âœ… Monitorear costos y tiempos
3. âœ… Ajustar prompts segÃºn feedback

---

## MÃ©tricas de Ã‰xito

### Calidad
- âœ… Cada segmento es coherente temÃ¡ticamente
- âœ… No hay cortes en mitad de conceptos importantes
- âœ… TÃ­tulos y metadata Ãºtiles

### Performance
- âœ… Tiempo total < 3 minutos para 24k palabras
- âœ… Menos llamadas API (10 vs 30)
- âœ… Procesamiento paralelo eficiente

### Robustez
- âœ… Fallback automÃ¡tico si falla segmentaciÃ³n IA
- âœ… Manejo de errores por segmento
- âœ… ValidaciÃ³n de JSON response

---

## Preguntas Frecuentes

### Â¿Por quÃ© no procesar todo de una sola vez?
Aunque GPT-4.1 puede manejar 1M tokens, dividir en segmentos:
- Mejora la calidad del output (foco)
- Permite retry granular si algo falla
- Facilita progreso tracking
- Permite procesamiento paralelo futuro

### Â¿CuÃ¡ndo usar mÃ©todo programÃ¡tico?
- Contenido < 3k palabras (no justifica el overhead)
- Procesamiento batch donde velocidad > calidad
- Fallback si la segmentaciÃ³n IA falla

### Â¿Se puede cachear la segmentaciÃ³n?
SÃ­, futura optimizaciÃ³n: guardar el plan de segmentaciÃ³n en session_state
para permitir re-procesamiento sin re-analizar.

### Â¿Funciona con contenido diarizado?
SÃ­, el `intelligent_segmenter` puede detectar reuniones y recomendar
`meeting_processor` como agente, ademÃ¡s de segmentar por cambios de speaker.

---

## PrÃ³ximos Pasos

1. **Implementar el diseÃ±o** siguiendo el plan de Fase 1-4
2. **Testing exhaustivo** con diferentes tipos de contenido
3. **OptimizaciÃ³n de prompts** basada en resultados reales
4. **Caching inteligente** para re-procesamiento
5. **Procesamiento paralelo** de segmentos (futura optimizaciÃ³n)
