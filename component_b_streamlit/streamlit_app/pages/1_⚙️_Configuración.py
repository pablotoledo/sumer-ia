#!/usr/bin/env python3
"""
P√°gina de Configuraci√≥n
======================

Configuraci√≥n de proveedores LLM y par√°metros del sistema.
"""

import streamlit as st
import sys
from pathlib import Path

# A√±adir el directorio padre al path
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
sys.path.append(str(parent_dir.parent))
sys.path.append(str(parent_dir))

from components.config_manager import ConfigManager
from components.ui_components import (
    setup_page_config, show_sidebar, show_provider_form, 
    show_config_status
)

def main():
    """P√°gina principal de configuraci√≥n."""
    
    setup_page_config()
    
    # Inicializar config manager
    if 'config_manager' not in st.session_state:
        st.session_state.config_manager = ConfigManager()
    
    config_manager = st.session_state.config_manager
    
    show_sidebar()
    
    st.title("‚öôÔ∏è Configuraci√≥n del Sistema")
    
    # Tabs para diferentes secciones
    tab1, tab2, tab3, tab4 = st.tabs([
        "üîó Proveedores LLM", 
        "üéØ Modelo Por Defecto", 
        "‚è±Ô∏è Rate Limiting", 
        "üîß Avanzado"
    ])
    
    with tab1:
        show_providers_config(config_manager)
    
    with tab2:
        show_default_model_config(config_manager)
    
    with tab3:
        show_rate_limiting_config(config_manager)
    
    with tab4:
        show_advanced_config(config_manager)

def show_providers_config(config_manager):
    """Muestra configuraci√≥n de proveedores LLM."""
    
    st.header("Configuraci√≥n de Proveedores LLM")
    
    # Estado general
    show_config_status(config_manager)
    
    st.markdown("---")
    
    # Selecci√≥n de proveedor a configurar
    provider_options = {
        "azure": "üü¶ Azure OpenAI",
        "generic": "ü¶ô Ollama (Local/Remoto)", 
        "openai": "üü¢ OpenAI",
        "anthropic": "üü£ Anthropic Claude"
    }
    
    selected_provider = st.selectbox(
        "Selecciona el proveedor a configurar:",
        options=list(provider_options.keys()),
        format_func=lambda x: provider_options[x]
    )
    
    if selected_provider:
        st.markdown("---")
        current_config = config_manager.get_provider_config(selected_provider)
        
        # Estado actual del proveedor
        is_configured = config_manager.is_provider_configured(selected_provider)
        
        if is_configured:
            st.success(f"‚úÖ {provider_options[selected_provider]} est√° configurado")
        else:
            st.warning(f"‚ö†Ô∏è {provider_options[selected_provider]} no est√° configurado")
        
        # Formulario de configuraci√≥n
        show_provider_form(selected_provider, current_config, config_manager)
        
        # Informaci√≥n adicional por proveedor
        show_provider_info(selected_provider)

def show_provider_info(provider: str):
    """Muestra informaci√≥n espec√≠fica del proveedor."""
    
    if provider == "azure":
        with st.expander("‚ÑπÔ∏è Informaci√≥n sobre Azure OpenAI"):
            st.markdown("""
            **Azure OpenAI** es la opci√≥n recomendada para producci√≥n.
            
            **Ventajas:**
            - Mejor calidad en espa√±ol
            - Respeta el idioma del texto original
            - Mayor retenci√≥n de contenido
            
            **Configuraci√≥n requerida:**
            - API Key de tu recurso Azure OpenAI
            - URL base del recurso (ej: https://mi-recurso.cognitiveservices.azure.com/)
            - Nombre del deployment
            - Versi√≥n de la API
            
            **Nota sobre Rate Limiting:**
            Azure OpenAI tiene l√≠mites estrictos. Configurar delays apropiados es crucial.
            """)
    
    elif provider == "generic":
        with st.expander("‚ÑπÔ∏è Informaci√≥n sobre Ollama"):
            st.markdown("""
            **Ollama** permite usar modelos locales sin API keys.
            
            **Ventajas:**
            - Sin rate limits
            - Privacidad total (local)
            - Gratuito
            
            **Desventajas:**
            - Puede responder en ingl√©s aunque el input sea espa√±ol
            - Menor calidad general comparado con Azure
            
            **Configuraci√≥n:**
            - Para servidor local: `http://localhost:11434/v1`
            - Para servidor remoto: `http://IP_DEL_SERVIDOR:11434/v1`
            
            **Modelos recomendados:**
            - llama3.1 (general)
            - mistral (espa√±ol)
            - codellama (c√≥digo)
            """)
    
    elif provider == "openai":
        with st.expander("‚ÑπÔ∏è Informaci√≥n sobre OpenAI"):
            st.markdown("""
            **OpenAI** ofrece los modelos GPT originales.
            
            **Ventajas:**
            - Acceso a modelos o1/o3 con reasoning
            - API ampliamente soportada
            - Buena documentaci√≥n
            
            **Configuraci√≥n:**
            - Solo requiere API Key
            - Modelos disponibles: gpt-4o, gpt-4, o1-mini, o3-mini
            """)
    
    elif provider == "anthropic":
        with st.expander("‚ÑπÔ∏è Informaci√≥n sobre Anthropic"):
            st.markdown("""
            **Anthropic Claude** es excelente para an√°lisis y razonamiento.
            
            **Ventajas:**
            - Muy bueno para an√°lisis detallado
            - Contexto largo (200k tokens)
            - Excelente seguimiento de instrucciones
            
            **Modelos disponibles:**
            - haiku (r√°pido y econ√≥mico)
            - sonnet (balance calidad/velocidad)
            - opus (m√°xima calidad)
            """)

def show_default_model_config(config_manager):
    """Configuraci√≥n del modelo por defecto."""
    
    st.header("Modelo Por Defecto")
    
    current_model = config_manager.get_default_model()
    
    st.info(f"üéØ **Modelo actual**: `{current_model}`")
    
    # Opciones de modelo seg√∫n proveedores configurados
    model_options = []
    
    if config_manager.is_provider_configured('azure'):
        model_options.extend([
            "azure.gpt-4.1",
            "azure.gpt-4o", 
            "azure.gpt-4"
        ])
    
    if config_manager.is_provider_configured('generic'):
        model_options.extend([
            "generic.llama3.1",
            "generic.mistral",
            "generic.codellama"
        ])
    
    if config_manager.is_provider_configured('openai'):
        model_options.extend([
            "gpt-4o",
            "gpt-4",
            "o1-mini",
            "o3-mini"
        ])
    
    if config_manager.is_provider_configured('anthropic'):
        model_options.extend([
            "haiku",
            "sonnet", 
            "opus"
        ])
    
    if not model_options:
        st.warning("‚ö†Ô∏è No hay proveedores configurados. Configura al menos uno en la pesta√±a 'Proveedores LLM'.")
        return
    
    # Selecci√≥n de nuevo modelo
    new_model = st.selectbox(
        "Selecciona el modelo por defecto:",
        options=model_options,
        index=model_options.index(current_model) if current_model in model_options else 0
    )
    
    if st.button("üíæ Actualizar Modelo Por Defecto"):
        config_manager.set_default_model(new_model)
        st.success(f"‚úÖ Modelo por defecto actualizado a: `{new_model}`")
        st.rerun()
    
    # Informaci√≥n sobre modelos
    with st.expander("‚ÑπÔ∏è Informaci√≥n sobre Modelos"):
        st.markdown("""
        **Azure OpenAI:**
        - `azure.gpt-4.1`: √öltimo modelo, mejor rendimiento
        - `azure.gpt-4o`: Optimizado para velocidad
        - `azure.gpt-4`: Modelo est√°ndar
        
        **Ollama:**
        - `generic.llama3.1`: Modelo general, buena calidad
        - `generic.mistral`: Especializado en espa√±ol
        - `generic.codellama`: Optimizado para c√≥digo
        
        **OpenAI:**
        - `gpt-4o`: Optimizado para velocidad y multimodal
        - `o1-mini`: Con capacidades de reasoning
        - `o3-mini`: √öltima generaci√≥n con reasoning avanzado
        
        **Anthropic:**
        - `haiku`: R√°pido y econ√≥mico
        - `sonnet`: Balance perfecto
        - `opus`: M√°xima calidad y capacidad
        """)

def show_rate_limiting_config(config_manager):
    """Configuraci√≥n de rate limiting."""
    
    st.header("Configuraci√≥n de Rate Limiting")
    
    st.info("‚ö° Estos par√°metros controlan c√≥mo el sistema maneja los l√≠mites de las APIs.")
    
    current_config = config_manager.get_rate_limiting_config()
    
    with st.form("rate_limiting_form"):
        st.subheader("Par√°metros de Rate Limiting")
        
        col1, col2 = st.columns(2)
        
        with col1:
            max_tokens = st.number_input(
                "Max Tokens por Request",
                value=current_config.get('max_tokens_per_request', 50000),
                min_value=1000,
                max_value=100000,
                step=1000,
                help="M√°ximo n√∫mero de tokens por request individual"
            )
            
            requests_per_minute = st.number_input(
                "Requests por Minuto",
                value=current_config.get('requests_per_minute', 3),
                min_value=1,
                max_value=60,
                help="M√°ximo n√∫mero de requests por minuto"
            )
        
        with col2:
            max_retries = st.number_input(
                "M√°ximo de Reintentos",
                value=current_config.get('max_retries', 3),
                min_value=0,
                max_value=10,
                help="N√∫mero m√°ximo de reintentos en caso de error 429"
            )

            delay_between_requests = st.number_input(
                "Delay entre Requests (segundos)",
                value=current_config.get('delay_between_requests', 30),
                min_value=0,
                max_value=300,
                help="Tiempo de espera entre requests consecutivos (proactivo)"
            )
        
        retry_base_delay = st.number_input(
            "Delay Base para Reintentos (segundos)",
            value=current_config.get('retry_base_delay', 60),
            min_value=10,
            max_value=300,
            help="Tiempo inicial de espera en el primer reintento (se duplica exponencialmente)"
        )
        
        # Presets comunes
        st.subheader("Presets Comunes")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.form_submit_button("üêå Conservador (S0 Tier)"):
                max_tokens = 30000
                requests_per_minute = 2
                max_retries = 5
                delay_between_requests = 45
                retry_base_delay = 90

        with col2:
            if st.form_submit_button("‚öñÔ∏è Balanceado"):
                max_tokens = 50000
                requests_per_minute = 5
                max_retries = 3
                delay_between_requests = 20
                retry_base_delay = 60

        with col3:
            if st.form_submit_button("üöÄ Agresivo"):
                max_tokens = 80000
                requests_per_minute = 10
                max_retries = 2
                delay_between_requests = 10
                retry_base_delay = 30
        
        submitted = st.form_submit_button("üíæ Guardar Configuraci√≥n")
        
        if submitted:
            new_config = {
                'max_tokens_per_request': max_tokens,
                'requests_per_minute': requests_per_minute,
                'max_retries': max_retries,
                'retry_base_delay': retry_base_delay,
                'delay_between_requests': delay_between_requests
            }

            config_manager.update_rate_limiting_config(new_config)
            st.success("‚úÖ Configuraci√≥n de rate limiting guardada")

def show_advanced_config(config_manager):
    """Configuraci√≥n avanzada."""
    
    st.header("Configuraci√≥n Avanzada")
    
    # Exportar configuraci√≥n
    st.subheader("üîß Exportar/Importar Configuraci√≥n")
    
    if st.button("üì§ Exportar Configuraci√≥n Actual"):
        config_json = config_manager.export_config_json()
        st.download_button(
            label="üíæ Descargar configuraci√≥n.json",
            data=config_json,
            file_name="fastagent_config.json",
            mime="application/json"
        )
    
    # Reset configuraci√≥n
    st.subheader("üîÑ Reset Configuraci√≥n")
    
    st.warning("‚ö†Ô∏è **Cuidado**: Esto borrar√° toda la configuraci√≥n actual.")
    
    if st.button("üóëÔ∏è Resetear a Valores Por Defecto"):
        if st.button("‚úÖ Confirmar Reset", type="primary"):
            config_manager.reset_to_defaults()
            st.success("‚úÖ Configuraci√≥n reseteada")
            st.rerun()
    
    # Informaci√≥n del sistema
    st.subheader("üìä Informaci√≥n del Sistema")
    
    config = config_manager.get_config()
    validation = config_manager.validate_config()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric(
            "Proveedores Configurados",
            len([p for p in ['azure', 'generic', 'openai', 'anthropic'] 
                if config_manager.is_provider_configured(p)])
        )
        
        st.metric(
            "Modelo Por Defecto",
            config.get('default_model', 'No configurado')
        )
    
    with col2:
        st.metric(
            "Rate Limiting",
            "‚úÖ OK" if validation['rate_limiting_ok'] else "‚ùå Error"
        )
        
        st.metric(
            "Configuraci√≥n V√°lida",
            "‚úÖ S√≠" if all(validation.values()) else "‚ùå No"
        )
    
    # Debug: mostrar configuraci√≥n completa
    with st.expander("üîç Ver Configuraci√≥n Completa (Debug)"):
        st.json(config)

if __name__ == "__main__":
    main()